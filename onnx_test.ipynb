{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from PIL import Image\n","import torch\n","import torch.nn as nn\n","from transformers import CLIPProcessor, CLIPModel\n","import onnxruntime"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["image = Image.open(\"./CLIP.png\")\n","processor = CLIPProcessor.from_pretrained(\"./models/openai/clip-vit-base-patch32\")\n","pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 3, 224, 224])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["pixel_values.size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# 测试onnxruntime\n","onnx_session = onnxruntime.InferenceSession(\"image_onnx_with_text.onnx\")\n","prob = onnx_session.run([\"logit\"], {\"pixel_values\": pixel_values.numpy()})"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[25.0325, 19.4092, 18.5067]]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["prob_ = torch.tensor(prob)\n","prob_"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[0.9949, 0.0036, 0.0015]]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["torch.softmax(prob_,dim=-1)"]},{"cell_type":"markdown","metadata":{},"source":["## 直接计算结果"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["texts = [\"a diagram\", \"a dog\", \"a cat\"]\n","model = CLIPModel.from_pretrained(\"./models/openai/clip-vit-base-patch32\")\n","input_data = processor(text=texts, images=image, return_tensors=\"pt\")\n","output = model(**input_data).logits_per_image\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[25.0325, 19.4092, 18.5067]], grad_fn=<TBackward0>)\n"]}],"source":["print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"clip","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
